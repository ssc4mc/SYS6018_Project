---
title: "Using TextRank to Identify Keywords in Text"
author: "Summer Chambers, Gaurav Anand, Vasudha Manikandan, and John Zhang"
date: "December 2020"
output: html_document
---


# Using TextRank to Identify Keywords in Text

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("textrank")
#install.packages("tm")
#install.packages("udpipe")
#install.packages("readtext")
#install.packages("wordcloud")
#install.packages("ggraph")
```

Some of the libraries you will need for this quick R tutorial are `textrank` `udpipe`, `readtext`, `wordcloud`, `igraph`, `ggraph`,`tidyverse`. They can be downloaded from CRAN using the following code.


```{r install_libraries, eval=FALSE}
install.packages("textrank")
install.packages("tm")
install.packages("udpipe")
install.packages("readtext")
install.packages("wordcloud")
install.packages("ggraph")
```

Now we can load the required libraries in order to begin working on ranking our text.

```{r message=FALSE, warning=FALSE}
library(textrank)
library(udpipe)
library(readtext)
library(wordcloud)
library(igraph)
library(ggraph)
library(tidyverse)
```

Example Text:


Let's read in the text! I converted everything to lowercase for simplicity of preprocessing.

```{r}
#Using TM
#filepath <- getwd()
#corp_cms <- Corpus(DirSource(filepath, pattern="CMS-2018-0101-0001.txt"))

#Readtext reads in text
cms <- readtext("mcdanks.txt")
cms <- tolower(cms$text) #make all lowercase
```

Now let's do some preprocessing of the text.  UDPipe's `annotate` function performs tokenization, lemmatization, and part of speech tagging.  I'll also remove stopwords by eliminating the first 20 most common non-nouns in the text.

```{r}
#Udpipe
#ud_model <- udpipe_download_model(language="english")
#ud_model <- udpipe_load_model(ud_model$file_model)
ud_model <- udpipe_load_model(file="english-ewt-ud-2.5-191206.udpipe")

x <- udpipe_annotate(ud_model, x=cms, tagger="default", parser="none") #tokenize, lemmatize, and tag POS
x <- as.data.frame(x)

unique(x$upos) #parts of speech

n_distinct(x$lemma)
```


```{r}
nouns_only <- subset(x, upos %in% "NOUN") #nouns only
noun_frequency <- txt_freq(x=nouns_only$lemma) #udpipe

non_nouns <- subset(x, !(upos %in% "NOUN")) #only non-nouns

stop_words <- head(txt_freq(non_nouns$lemma), 20)

stop_words$key
```

Now let's use TextRank to extract keywords. They recommend restricting our vocabulary to Nouns and Adjectives only for the most accurate results.

```{r}
#Textrank
clean_x <- subset(x, !(lemma %in% stop_words$key))

textrank_result <- textrank_keywords(clean_x$lemma, 
                              relevant=clean_x$upos %in% c("NOUN", "ADJ"), 
                              ngram_max=8, 
                              sep=" ")


summary(textrank_result$keywords_by_ngram)

log_sorted_pr <- sort(log(textrank_result$pagerank$vector), decreasing=T)

plot(log(textrank_result$pagerank$vector), xlab="Word Index", ylab="Log PageRank", main="TextRank")
abline(a=log_sorted_pr[(length(log_sorted_pr)/3)], b=0)


```


We can plot any subset of these identified keywords by their frequency with WordCloud.

```{r}

keywords_frequent <- subset(textrank_result$keywords, freq > 5)# & ngram = 1 )

head(keywords_frequent$keyword, 20)

#Wordcloud
wordcloud(words=keywords_frequent$keyword, freq=keywords_frequent$freq, colors=c())

```   


Cool, right?  But how does TextRank work?

It's a graphically-based model which generates a network of nodes as words and edges as co-occurrences of words. Using a variation of the PageRank algorithm, it ranks the most important or graphically central nodes highest.  PageRank was created by Google to measure the importance of webpages by understanding how many other websites linked to them.  The PageRank algorithm relies on the idea of a random surfer clicking from page to page, along with a dampening factor representing a totally random jump to any page in the network.  This equation represents the algorithm:


$PR(V_i) = (1-d) + (d \times \sum_{j \in to(V_i)}{\frac{PR(V_j)}{from(V_j)}})$

According to the authors of both PageRank and TextRank, the dampening factor <d> is chosen to be .85, resulting in the equation:

$PR(V_i) = 0.15 + (0.85 \times \sum_{j \in to(V_i)}{\frac{PR(V_j)}{from(V_j)}})$

The TextRank algorithm begins by setting every node's score equal to an arbitrary value, then iterates until it converges on a solution/final graph.  This process tends to take about 20-30 iterations.

Sequential keywords are reconstructed as multi-word keywords after the iterations are complete.  The function returns the top 1/3 sorted importance scores and keywords.



Here we'll use igraph to visualize and explain the algorithm as it is used in text.


First, we can look at the most common word collocations (how often to words appear sequentially?)

```{r}
x_keywords <- subset(x, lemma %in% keywords_frequent$keyword)
colloc <- keywords_collocation(x_keywords, term="lemma", group="sentence_id", ngram_max=2, n_min=75)

colloc


#Weighted Undirected

#code from https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html

cooc <- cooccurrence(x=subset(clean_x, upos %in% c("NOUN", "ADJ")), term="lemma", group="sentence_id")


network <- head(cooc, 45)
network <- graph_from_data_frame(network)
ggraph(network, layout="fr") +
  geom_edge_link(aes(width=cooc, edge_alpha=cooc), edge_colour="purple") +
  geom_node_text(aes(label=name), col="darkblue", size=4) +
  theme(legend.position="none") +
  labs(title="Common Word Cooccurrences")
```

## Sentence Extraction

We can also us TextRank for sentence extraction where we can automatically identify the most important sentences in a document. Using TextRank, we can calculate the overlap between any two sentences in the document by the number of common tokens between their processed forms. 

The similiarity between any two sentences, $S_i$ and $S_j$ can be calculated using the similiarity score,

$$
\text{Similiarity}(S_i,S_j) = \frac{ \{ w_k \mid w_k \in S_i \text{ and } w_k \in S_j \} }{log(|S_i|) + log(|S_j|)} 
$$

This similiarity score allows us to represent sentences as nodes in graphs connected to other sentences based on how similiar they are to those nodes. This weighted graph-based ranking can then analyzed using the `textrank` module in R.



```{r sentence_extraction}

```


References: 

* https://bnosac.github.io/udpipe/docs/doc7.html  
* https://rstudio-pubs-static.s3.amazonaws.com/341868_231c841ed2d1476c9ccb3b7a07596a8c.html   
* https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf  
* https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html 
