knitr::opts_chunk$set(echo = TRUE)
#install.packages("textrank")
#install.packages("tm")
#install.packages("udpipe")
#install.packages("readtext")
#install.packages("wordcloud")
library(textrank)
library(tm)
library(udpipe)
library(readtext)
library(wordcloud)
library(igraph)
library(tidyverse)
#Using TM
#filepath <- getwd()
#corp_cms <- Corpus(DirSource(filepath, pattern="CMS-2018-0101-0001.txt"))
#Readtext reads in text
cms <- readtext("CMS-2018-0101-0001.txt")
cms <- tolower(cms$text) #make all lowercase
cms <- substr(cms, 48111, 929040) #select a substring
#Udpipe
#ud_model <- udpipe_download_model(language="english")
#ud_model <- udpipe_load_model(ud_model$file_model)
ud_model <- udpipe_load_model(file="english-ewt-ud-2.5-191206.udpipe")
x <- udpipe_annotate(ud_model, x=cms, trace=TRUE) #use udpipe to tokenize and tag POS
x <- as.data.frame(x)
unique(x$upos) #parts of speech
nouns_only <- subset(x, upos %in% "NOUN") #nouns only
noun_frequency <- txt_freq(x=nouns_only$lemma) #udpipe
non_nouns <- subset(x, !(upos %in% "NOUN")) #only non-nouns
stop_words <- head(txt_freq(non_nouns$lemma), 20)
stop_words$key
#Textrank
clean_x <- subset(x, !(lemma %in% stop_words$key))
identified_keywords <- textrank_keywords(clean_x$lemma,
relevant=clean_x$upos %in% c("NOUN", "ADJ"),
ngram_max=8,
sep=" ")
keywords_frequent <- subset(identified_keywords$keywords, freq > 80)# & ngram = 1 )
head(keywords_frequent$keyword, 20)
#Wordcloud
wordcloud(words=keywords_frequent$keyword, freq=keywords_frequent$freq, colors=c())
View(keywords_frequent)
